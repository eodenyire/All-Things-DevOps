
# Containers 


- [What are Containers?](#what-are-containers)
- [Why Use Containers?](#why-use-containers)
- [How to Containerize an App](#how-to-containerize-an-app)
- [Microservices](#microservices)
- [Container Orchestration](#container-orchestration)
- [Container Security](#container-security)
- [Container Logging and Monitoring](#container-logging-and-monitoring)
- [Container Systems and OSs](#container-systems-and-oss)


## What are Containers?

So what is a container? One way to think about them is that they're lightweight virtual machines. With a virtual machine, you have to virtualize an entire operating system as well as the software you want to run. This makes VMs really resource heavy.

The operating systems is often the single largest most resource-intensive piece of hardware on your computer and so running multiple OS's on the same computer, just so that you can have separate environments, uses a lot of your resources. To overcome this issue, the Linux operating system began implementing containers. The idea is simple, if you're running a Linux OS on your computer already, why run a new OS for each VM? Instead you can use the core of the OS, called the kernel, for each VM. This way the VMs only run the software that they need to.

The difficulty with this is that it is important that the VMs not be able to affect each other, or the underlying computer they're running on. And containers need to replicate this functionality. So the Linux team had to implement some safety features into the kernel itself. Features such as being able to block off different parts of the kernel processor and memory for the different containers, so that the code running on one container can't accidentally access another container through the kernel. Now that these containers were implemented at the kernel level, any amount of software could be run inside of one and it would be like running it in its own VM, or own physical machine. And because all Linux distros share the same fundamental Linux kernel, you can easily run containers with different distros, just as easily as you can run containers using the same distro.

The software that makes each distribution unique all runs on top the kernel and it's only the kernel that is shared across all the containers and the host OS. Once containers are implemented at the most basic fundamental part of the Linux OS, software which made it easier to implement these Linux containers begin to pop up. One of the first and most successful container software projects is called Docker. Docker makes it easy to define, manage and use Linux containers by simply writing plain text documents to define the software that you want running inside of a particular container.

In addition, Docker and other companies began building software that could link containers together into a single app, as well as orchestrate spinning them up and down in the cloud rapidly. In addition to Docker, there are other container systems. But I'm mostly going to use Docker as the example of how containers work in this course, because it is the most frequently used and frankly the most easily explained of all these systems.

As an example, a Dockerfile is used to define a container. This Dockerfile starts with a standard image, usually provided by a software project, such as, a Linux distro or web technology like node.js. From there you can add new pieces to that image in a certain order, usually by running commands telling the image to install and setup new software. Once the file is written and saved, it can be sent in plain text to any two other people and built in just a few seconds on any computer that has Docker installed.

This is very different from VMs, where you have to send a multi-gigabyte executable file to other people who want to run the VM. Here you're just sending a few kilobytes of instructions on how to build the container yourself. Docker builds these images in layers, for instance, if you were to run a Docker build on the Stocker file, first Docker would take the official Ubuntu image, then it would run apt-get install-y software-properties-common-python to create a new image, and then from that image, it would run add-apt-repository et cetera, to create another image, and so on, and so on.

Once the Docker file is built, you can run the image inside a container, or copy it to multiple times, to run it in as many containers as you want. For instance, here there are four instances of container A and two instances of container B. Further software can be used to network containers to each other, the same way VMs or physical machines can be networked together. So that your containers can communicate with each other to create one large system built with many small containers. Here we have a fairly standard networking arrangement, where all the containers live in one VPN, and load balancers direct the traffic in into each subnet towards the least used container at the moment.

The details of how this is implemented may change depending on what sort of system your containers are running on. And we'll dive deeper into the specifics in a later lesson. But this should give you an idea of how to use them. But that's just on Linux. What if you want to run containers on another operating system?

While Docker lets you run Linux containers on Mac or Windows by first starting a really lightweight Linux VM that mostly just runs the kernel, and then running all the other containers inside that VM. So this is slower than running Linux containers on the Linux, because you do have a VM, but it's faster than the old paradigm of using a bunch of VMs, because you're only running one, and you get the other benefits of containers along with it. In addition, Microsoft has been working to build Windows containers.

These are containers that are built into the Windows operating systems, so that instead of running a Linux distro and a container, you can run Windows in Windows software in a container. Windows has been working really closely with Docker on this project, so they work with Docker. However, running a Windows container on Linux or Mac doesn't really work at this point.

Finally, there's currently no way to run Mac OS containers. It's just not something Apple has implemented in the Mac OS kernel, and since Mac OS is almost never used to run servers, no one is really asking for it. There are container systems for other operating systems, like BSD, but that's a bit out of scope for this course, since they're rarely used commercially. So that's what containers are. A way to run multiple computers on a single machine. Each with a different operating system software installed, in the fast and secure manner. In the next lesson we'll talk about why you'd want to use containers. And then show you how to containerize a app.

## Why Use Containers?

So if containers are this big, new, complicated paradigm to learn, why bother with it? Slightly faster run times than VMs are nice, although these days, powerful computers are so cheap that it might be cheaper to buy faster servers than to train a development team on how to use containers.

The thing with containers is that their benefits do not stop at being faster than VMs; they start there. A development team might indeed even work faster and more efficiently once they're comfortable with using containers. This is because containers can streamline and even make possible various development and operations tasks, such as testing, DevOps, and continuous deployment. I've seen it argued very well that the biggest benefit to containers is not the security or speed of your app, but rather, the speed with which you can build, test and deploy new features when using them.

So we'll talk about how containers can help you, both from a technological standpoint, but also from a management perspective.

The first thing that containers enable is fast testing. Containers can be built and spun up very quickly, and code inside a container will run the same pretty much no matter what computer it's running on. As such, if you build an app with containers, every time you check in code, you can quickly build a new container with that code and play around with it. If you run your app in containers in production, then the containerized app that a developer has on their computer is exactly the same as the app that will run on the cloud.

Gone are the days of each developer managing the dependencies on their computer to ensure that the app that they're working on can run locally. This quick feedback loop for your developers can lead to better code being written, and for less need for troubleshooting when strange bugs pop up that are caused by the developer having a slightly different version of the software on their computer than that you're running in production. In addition, this makes it easier for others to get up and running with your code as well.

For instance, an open-source project might include a Docker file as part of their GitHub repo. This way, anyone who downloads the code to their machine in order to edit it can get a version up and running to test their changes with just a few Docker commands, whereas before, the developer would have to set up a new VM on their machine which followed a number of instructions on how to get exactly the software that the open-source project needs to run. Now they only need to run one or two commands and the rest is done automatically by Docker.

For instance, this small open source project makes it easy to test the project by spinning up containers for the databases, so you don't have to install a network the database software on your host OS. This can be useful within a company as well. You can quickly get non-technical people testing new software once they learn the basic Docker commands. Imagine a design or marketing team that feels empowered to test and give feedback on your alpha builds without needing help from the developers. Simply send them a new Docker file and ask them to build and run it. Containers are also really useful when it comes to the DevOps process.

DevOps is a way of doing development work where the developer and IT teams work closely together, and as much of the process is automated as possible. For instance, with continuous deployment, any accepted change to the main code base is deployed to the users, and this process is made much easier with containers. Container systems are immutable, which makes upgrading them easy. Without containers, you may have to deploy the new code by taking machines offline, updating the app on that machine, and then spinning it up again, doing this for each machine until they're all running the same software. With containers, you simply build a new image with the new code and push that out to your cluster. You never update a container itself because they are unchangeable or immutable.

Your orchestration software will direct new traffic to the new containers, and as the old containers are no longer needed, because they're no longer seeing new traffic, they'll be shut down. This way, your users experience a seamless transition to the new software, and you don't have to worry about periods of having fewer servers online during the transition. Docker, in particular, is very easily automated, having been designed with that automation in mind. It's easy to plug Docker into your CICD process so that containers are built from new code, tested and then pushed into production all automatically. Finally, once an app is up and running, this same orchestration software makes scaling the app to the amount of traffic it's seeing very easy. It's kind of like a load balancer in a typical cloud setup.

The orchestration software will look at what containers are getting sluggish from too much traffic and will spin up more of that kind of container. If you've built your app using microservices, as is suggested when containerizing an app, then the orchestration software has a lot of fine detailed control over what gets spun up or down.

Maybe only your database server needs more help. The front end is fine. It takes some time for teams to containerize an app and update their workflow to work best with containers, but once they do, they should be writing better code, getting more feedback from different parts of the organization, and deploying and managing the apps more easily. In the next lesson, we'll explore what it actually takes to containerize an app.

## How to Containerize an App

Let's talk quickly about how to containerize an app. To be clear, this isn't going to be a tutorial or a lab. I'm not going to walk through containerizing an actual app. However, I'll help you understand the big concepts around containerization and hopefully give you enough that you need to get started.

Containerizing an app isn't as simple as writing a dockerfile that includes instructions on how to get the whole app up and running in a container. I mean, you can do that. It's possible, but if you do, you're not going to get most of the benefits of containers. Containers work best when they're small and do one thing well. This means that you'll want an app to be made out of Microservices. Microservices is when your app is built in independent pieces.

These pieces might written in different programming languages, might have different dependencies. They might even run on different operating systems. All your services need to be written with APIs. This way any individual service can communicate with any other through their API. For instance, a database service exposes an API that let's other services request and write data to it. Then these microservices speak to each other over a network or often over multiple sub networks. Let's take a look at what this means. In an old school physical server, you'd probably be running a LAMP stack.

Each server is running Linux which is running Apache, MySQL, and either PHP or Python. Thus, you're OS, server and networking database and app are all running on the same physical hardware. In a modern, Cloud deployment, we'd split some of those things up. The networking and server functions are handled by the info structure and MySQL and your app are running on different types of Cloud instances, but your entire app might still live on one EC2 instance. For instance, if you're using AWS. In a containerized microservices app, things are split apart even more.

Each different part of your app runs in a different container, so your front end code is separate from your back end code. You might have a microservice that logs people into your app, and then a different microservice to handle communication between users, and yet another microservice to handle shopping and checkout. Microservices do not require containers. You could build out microserviced based app on AWS using EC2 instances.

Containers both make microservices easier and require microservices to get the full benefits from that. Why does your containerized app need to be built as microservices? A few reasons The first is that not everything can be containerized. Containers are designed to be spun up or spun down at anytime. They are ephemeral as opposed to persistent; however, most apps needs to have data that are persistent. You don't want your users to lose all their data every time the container they're on shuts down. You shouldn't be storing any data inside your containers.

The database software might run in a container, but the data has to be stored in a different volume. Another aspect of containers being ephemeral is that they should be stateless. If one container goes down, that should not effect your users at all because any information about their state should be stored outside of the app in data storage.

For instance, if a user is three steps through a five step purchase process and their container they're using goes down, their traffic should seamlessly be picked up by another container and should put them on the fourth step with their payment info, shopping cart, and everything else intact when they move forward.

The reason you want to design your app this way is that your microservices can be scaled horizontally. This means that when you have more traffic hitting your app you can simply spin up more of whichever services are getting a heavy load just like using a load balancer with a Cloud server instances. Just like in that case, your app needs to be designed in such a way that individual instances going up and down won't effect your users.

A place where containers may differ from physical and Cloud servers is that they should generally be immutable. This means that you never want to update your containers themselves. If you have new code to push, you want to do that by spinning up new containers with that code, putting those on the network, and then spinning down the old containers rather than simply pushing that code to the containers that are already running. If you design your app with ephemeral stateless containers in mind this should come fairly easily. It's more of a process change than a technical one. Immutability is a key concept of running containerized systems.

All of this is a lot to keep in mind, so there's a framework for building containerized microservice apps that a lot of businesses use. This is called the 12-Factor Application Framework. These are 12 principles that your app should always follow. It's a guide for how to build your app and what you're building towards. The factors suggest how to treat logs, how to expose microservices to each other, that you should keep your services stateless, and more.

It's a little outside the scope of this course to go through all 12 factors, but a Google search for 12-Factor App will give you a lot more information. Container companies like Docker explicitly suggest that the best containerized apps are 12-Factor Apps. That's what you need to know to get started containerizing an app. It's a large process that requires both technical changes to the app as well as process changes to the way your team works. As we discussed in previous lessons, once the process is finished you can reap many rewards.

## Microservices

Containers and microservices tend to come hand in hand. While having a large app wrapped in a container can be useful for some testing, to get the benefits out of using containers in production you'll need to architect your app with containers in mind. In this lesson, we'll go over a popular method for doing that called 12-Factor Apps and give you an example of a microservice app in the wild.

Here's a brief review from our Intro to Containers course on what a microservice app architecture would look like. If you're still unclear, you might want to go back to that course before taking this one. 12-Factor Apps are a standard way of architecting an app as microservices. The 12 factors are like a checklist for each microservice that makes it easier to ensure that your app is healthy and all of your services will work together.

The 12-Factor App manifesto was originally developed by Heroku, an app-hosting platform. But a lot of other infrastructure startups, including Docker, the main container software, explicitly work towards supporting 12-Factor Apps now. We'll go through each of the 12-Factor Apps, quickly stating what each one means in practice. This won't be a tutorial for building them, but you can find plenty more resources for that online.

Factor one: The codebase.

There should be one codebase, tracked in revision control, with many deploys. This means that you should be using version control software like Git, or ideally GitHub or another code-hosting platform. You should track all the changes that everyone makes and they should be easy to roll back. Any modern software development system relies on version control at this point. So this doesn't apply just to containerized or 12-Factor Apps.

In addition, you should be frequently testing the code you check-in to the version control system, preferably with automated tests.

Factor two: Dependencies. Explicitly declare and isolate dependencies. This means that if your code depends on something to run properly, whether in a library, a specific database, or another app, that should be explicit in the code and easy to setup.

Containers make this easy because with systems like Docker Compose you can define all the dependencies and get them up and running in other containers with a single command.

Factor three: Config. Store the config in the environment. The idea is that anyhting that may vary between environment should be stored in that environment, probably as config files.

This would be stuff like logins, IPs of machines your app will talk to, that kind of thing. Docker will let you store some of this in, for instance, the Docker file, a config file, or a compose file. But people argue whether or not that means it's stored in the code. For our purposes, that's good enough.

Factor four: Backing services. Treat backing services as attached resources. Here any other resources that your code needs to talk to, whether they be a database, another microservice, an email service, or whatever, should be accessible via URL and maybe a username and password. It shouldn't matter whether the service is hosted on the same physical machine, the same cloud service, or even a completely different service. Your app should treat them all the same.

Factor five: Build, release, run. Strictly separate build and run stages. The big idea here is that starting and stopping an app should be simple and require little human intervention. The way it's suggested to do that is to separate build, release, and run stages. To build the app is to trim the code into executable pieces, whether those be binary, script, containers, or whatever. And this should probably be handled by the dev team. To release the code is to put the executable pieces on your production servers. And this is handled by DevOps or SysAdmins. Once an app is released, the run stage is as simple as running a script to turn on or shut down the app and requires no extra building or releasing.

Factor six: Processes. Execute the app as one or more stateless processes. Statelessness is a very important part of container apps. Any given container should be stateless. When a user is interacting with it, that user's state should be stored somewhere else, so that if that container goes down, they can continue right where they left off on a different container. Each step the user goes through should export all the necessary data for any other container to know right where that user was.

Factor seven: Port binding. Export services via port binding. This is the flip side of factor four because your service will be treated like an attached resource by other services. This means you should expose your service as a URL, which may be public or private, and usually means creating an API for other services to interact with yours, just like you interact with them.

Factor eight: Concurrency. Scale out via the process model. This is a fancy way of saying that you should scale by adding more servers and services, not by adding more resources to a given server. Or with containers, you scale by spinning up more containers of a certain type. A lot of this is handled by your orchestration software, which we'll explain in the next lesson.

Factor nine: Disposability. Maximize robustness with a fast startup and graceful shutdown. This is very important when designing for containers, which can be spun up or spun down at any time. Your services should be designed so that shutdowns and crashes don't affect the entire system, and starting up a service should be as fast as possible.

Factor 10: Development production parity. Keep development, staging, and production as similar as possible. This is actually a place where containers make it easier to follow the factor, rather than the other way around. One of the main benefits of containers is that they are the same across different machines. So if you're running code in containers on both dev and production, you shouldn't see many errors when moving code between the two.

Factor 11: Logs. Treat logs as event streams. This is frankly the least-relevant factor. Yes, it's important to capture logs and errors for long-term recording and there are tools that let you do that with containers. You'll more or less get this for free if you use any modern container-monitoring software, which we'll cover a few lessons from now.

Finally, factor 12: Admin processes. Run admin and management tasks as one-off processes. This is an important element of your development and management processes. If you want to, for instance, add data to the database, you should do that from the console on your app in production, not by running code directly on the database. This means designing your system so that developers can gain admin access to all of your services and can run custom jobs on them. This will ensure that you don't break the system by accidentally reintroducing poorly-formatted data at any point.

So those are the 12 factors and what they mean in plain language. Each of your services should be designed to match each of these factors as best as possible. And then they'll work together well. Now, let's suppose you have a 12-Factor microservices app. Can you just dump all of your services in containers by writing Docker files and be ready to go? Well, not quite.

First, you need to make sure that not everything is containerized. In particular, if you have any necessary stateful elements of your app, those probably shouldn't be put in a container. The best practice would be to eventually refactor them to be stateless, but until you do, keep them on a machine somewhere so they don't go up and down too much. In addition, data should never be stored in a container. Database software can be containerized, but the database itself needs to be stored on a mounted volume.

Finally, while containers are very secure, there are some applications which require a level of security that even containers cannot guarantee. As an example, when working with medical data.

## Container Orchestration

So far, we've talked about starting up individual containers or networking different types of containers to each other. But of course, one of the promised benefits of running your app on containers is that you can quickly scale out a single container by running multiple instances.

Doing this manually isn't really an option. You'd have to have many people constantly monitoring your web traffic, spinning up and down containers. No, you need to be able to do this automatically. This is where orchestration software comes in. In the world of containers, orchestration refers to managing a large set of containers, including horizontal scaling.

Orchestration software takes care of spinning up and down individual containers while monitoring the health of the containers, all across multiple physical or virtual servers. Generally, you define what your app should look like in an abstract way using YAML or JSON. From there, the orchestration software takes care of the underlying details of managing containers and the underlying physical resources they are running on. This splits apart the roles of architecting an app and architecting the hardware the app is running on.

For the devOps role, the hardware is just one deployment target that the orchestration software will handle. As Apache Mesos promises, you can program against your data center like it's a single pool of resources. There are generally two types of orchestration tools: those designed to run in your own data centers with you managing them and services for orchestrating in the cloud.

Many, but not all, of the cloud services are built on top of local tools, meaning that this distinction isn't perfect, but for our purposes, it will do. Let's look at the on-prem orchestration tools first. The most important to be aware of are Kubernetes, Mesosphere Marathon, and Docker Swarm. Kubernetes is a Google project and is one of the oldest and most popular orchestration tools, although that doesn't make it very old in the grand scheme of things.

Google uses a version of Kubernetes internally to manage their production apps. Kubernetes is now largely focused on orchestrating Docker containers, although it was originally built for Google's internal pre-Docker container technology, and can also manage Rocket containers. Kubernetes works with a master node paradigm. Each cluster is made up of a master server that runs Kubernetes and a number of node servers, which Kubernetes manages. Your container runs on the node servers, and Kubernetes manages which servers are running which specific containers. Next is Marathon for Apache Mesosphere. Mesosphere is touted as a data center operating system, and Mesos is its underlying kernel. Mesosphere lets you treat your entire data center as a single entity. You run processes on Mesosphere, and Mesos decides which physical components to put towards that job without you having to think about it.

Marathon is the software package for Mesosphere that specifically handles container orchestration. Some very large companies, such as eBay, Verizon, and Twitter, really on Mesosphere. Finally, Docker has built orchestration technology directly into the core of its engine with its latest 1.12 release, which they call Swarm. This is a little confusing, because Docker Swarm used to be a standalone product, and the new standard Swarm mode is a set of features built right into the Docker engine and is slightly different than the old Swarm.

Swarm positions itself as being more lightweight and easy to use than Kubernetes or Mesos. It is a decentralized orchestration system where each node can manage itself and its own traffic instead of being managed by a central master node. In addition to these major on-prem orchestration tools, there are a number of cloud platforms that have container orchestration built in, some built on top of these tools and some offering their own interfaces.

For instance, AWS offers its EC2 Container Service, or ECS, which will manage a cluster of EC2 instances with Docker running on top of them. Rather than using a specific container orchestration system, ECS is built with common AWS tools like Elastic Load Balancer and IAM. Azure also offers a container service, ACS.

Google also offers container hosting and management with its Google Container Engine, or GKE system. As you might expect, GKE uses Kubernetes. Docker offers Docker Cloud, its own cloud container service. Docker Cloud uses Docker Swarm to manage hosted applications and also lets you perform other Docker-specific tasks in the cloud, such as image management and CICD and lets you manage your cloud account from Docker's desktop tools.

Docker Cloud is cloud hosting built from the ground-up for containers, rather than being container hosting added to an existing cloud hosting service. So that should give you an idea of what orchestration is and the main tools people use. The orchestration ecosystem is still new, and new tools pop up frequently, but this should give you an understanding of what these tools are and how they work.

## Container Security

Using containers in production introduces a new set of security concerns into play. In this lesson, we'll talk about those and how to mitigate some of them. To be clear, we'll be talking about container-specific security. Obviously, your code should follow best security practices as even the best secured physical server won't prevent someone from accessing your database if you don't sanitize your user inputs.

First, let's be clear, a system built with containers is on the whole, less secure than one built with virtual machines. Virtual machines have a hypervisor layer through which all commands and data must pass between the VM and the host machine. Containers, on the other hand, use the underlying kernel of the host machine to run commands. As such, even with the security features baked into the Linux kernel, there is no way to 100% lock down a container from the kernel, and anyone with root access to the host kernel will be able to peek into all the containers running on that machine.

As such, even with the security features baked into the Linux kernel, there is no way to 100% lock down the container from the kernel, and anyone with root access to the host kernel will be able to peek into all of the containers running on that machine.

But all is not lost. After all, containers can run inside VMs. So if you really wanted the operational benefits of containers but the security benefits of VMs and performance isn't your number one concern, you can just run your containers inside VMs on the host. This gives you the security, but also the poor performance of a hypervisor layer, but all the other benefits of Docker.

As an example, this is how AWS container services work. You run the containers inside EC2 instances which are just hypervisor VMs running on AWS's physical servers. In addition, companies like Intel and VMware are working on building very fast container-focused VMs to minimize the performance impact while getting all the security from the VMs.

However, more and more companies are finding that this isn't necessary. Rather, as the attack factors for containers apps are discovered and secure, container apps are becoming more secure to run on bare metal. For instance, IBM's Bluemix cloud hosting, an AWS competitor, lets you run containers directly on their cloud servers. Regardless of how you run your containers, there are some security best practices to follow. The first and perhaps most important is to only run verified images so that no attacker can inject their image into your system.

The best way to do this is to have an image registry for your company which lets you build, share, track, and run your images. An image registry is kind of like GitHub but for container images. Image registries like Docker Hub and Docker Trusted Registry implement a tool called Docker Content Trust which lets you sign your images and define exactly what images are allowed to be run on your system. In addition, there are security scanning tools like Atomic Scan and Docker Security Scanning which you can run on your images once they've been built.

These can be automated and implemented as a part of your continuous integration or testing processes. They give you a bill of health for each image, letting you know if they have any known vulnerabilities. When creating your images, it's important that they follow the principle of least privilege. The container should have the least amount of privileges it needs to do its job. For many containers, this means implementing read-only file systems.

Even if an attacker gets full access to that container, they won't be able to write anything to the underlying system. In addition, you can do things like limit resources for each container so they can't be hijacked for other purposes. And of course, don't run your processes in the container as a root user. Finally, there are runtime threat detection systems that you can look into like Aqua Security and Red Hat OpenShift. These take a baseline of what your system should look like and moderate it to make sure that nothing out of the ordinary happens.

If it seems unusual activity or traffic, you can set up rules so they alert you, shut the system down, or take other steps. This way, if an attacker gains access to your system, you're more likely to notice right away. Containers still aren't the most secure system, and for certain incredibly high risk applications like healthcare data, they might not be right. However, for most apps there are simple steps that you can take to secure your system just as well as any other production environment you're likely to use.

## Container Logging and Monitoring

Once your containerized app is up and running, you'll want to be able to monitor its health. This means both monitoring the uptime of the system and containers, and also tracking logs from inside your individual services. There are a number of tools on the market designed for container system monitoring. If you're using orchestration software, some of that monitoring is built into the system already.

After all, Kubernetes or Swarm can't spin up more containers to handle high traffic periods if they don't know what traffic is going where. Kubernetes in particular has a project called Heapster that aggregates a lot of different monitoring data into one spot. It largely wraps together different open source monitoring tools such as cAdvisor and InfluxDB. cAdvisor can be used with any Docker system and is a way to visually see stats about your individual Docker containers.

It will give you information on run time, CPU and memory usage, and more, all in pretty and easy-to-read graphs. InfluxDB is a way to watch data loads and read rights in a graphical interface. There are also third-party cloud apps for monitoring.

A popular one is Data Dog. With Data Dog, you instruct Docker to send all the information from a cluster to their service through the API, and they then display the information and let you custom build dashboards that you can drill down into your data in real time. You can also set up alerts on Data Dog, so that if, for instance, there are too many containers running on a certain host, or not enough of a specific type of container, you can be alerted through whatever channels you'd like.

These tools, along with others like Prometheus and Sysdig all query the health of your containers, but not what going on inside of them. As we mentioned in the 12 Factor lesson, you'll want to make sure that each of your services is exporting logs. The good news is that most tools that work without containers will work with containers here. You can built the logging into your app, then you can send those logs to a system like New Relic which will collect those logs and let you build custom dashboards out of them. Monitoring your containers isn't particularly hard if you're using tools designed just for that.

## Container Systems and OSs

Most of the containers we've been talking about so far are Linux containers, run using the Docker container engine. However, there are different engines for Linux containers, as well as containers that work with other operating systems. Here, we'll discuss these in brief, as well as discuss why Docker is the top choice for most people.

Docker is built on top of Linux containers, which are built directly into the Linux kernel. The kernel itself implements the ability to sandbox parts of the OS to be run as a container, but then Docker has built a bunch of functionality on top of that. Technically, Docker doesn't actually use Linux containers, but rather uses the tools built into the kernel for Linux containers such as cgroups and namespaces, and then rewraps them with its own interface. Without Docker, defining images would be a lot harder than writing a Docker file, and building them would be a lot harder than running Docker build and so on.

There are other Linux container systems other than Docker. For instance, the version of containers built into Linux itself is often called LXC. LXC containers are a lot harder to work with than Docker containers, and unlike Docker containers, aren't built with stateless, single process applications in mind. Rocket, or rkt, is another system which, like Docker, is built on top of the Linux container tools.

Rocket is managed by a company called CoreOS and focuses specifically on secure, cloud-based containers. In addition, there's the Open Container Initiative, a joint venture between Docker, CoreOS, Linux and other container industry companies and groups. The OCI defines standards for what containers should look like, so that the workflow between systems like Docker and Rocket will be the same. Docker and Rocket containers won't necessarily work on each other's systems, but they use generally the same paradigm for building, sharing and more, because of the OCI. The OCI also maintains its own runtime environment, runC, which is lightweight, and as they say, unopinionated. runC can be used in concert with Docker or OCI container systems.

Other, non Linux operating systems also implement containers. BSD is another free, open source OS that implements what it calls jails. Jails are like a combination of virtual machines and containers. They run at a deep system level, like containers, but they use virtualization in order to separate the parts so that they're more secure. However, BSD is much less commonly used than Linux, so you'll rarely see these in commercial environments. I also wanted to mention macOS here.

While macOS is built on top of a BSD kernel, called Darwin, macOS does not support jails or containers. Sure, you can run containers on macOS, but you can't run macOS inside a container. It's not something that Apple supports, and since macOS is mainly a consumer, and not server, operating system, it's not something the market is demanding either. On the other hand, there are Windows containers.

Windows containers come in two types. Windows Server Containers work just like Linux containers. They share a kernel with the host Windows Server through otherwise separate processes. Then you have Hyper-V containers, which are further isolated from the host Windows machine through a highly optimized virtual machine, which, as we mentioned, in the security lesson, leads to a more secure, but slightly less performative system.

Docker allows you to define, build, manage and run Windows containers, just like it does Linux containers. This means that you don't have to learn a new workflow to work with Windows containers if you already use Docker. Windows containers only run on machines running Windows 10 Professional, Windows 10 Enterprise or Windows Server 2016. Even on Windows 10, the OS your container will be running will be Windows Server. You can't run Windows 10 in a container. In addition, the only orchestration software that currently works with Windows containers is Docker Swarm. Although, as of the publishing of this course, both Kubernetes and Mesos are working on these features.